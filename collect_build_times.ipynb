{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Build Times\n",
    "\n",
    "Collect build times for datasets.\n",
    "\n",
    "How to run\n",
    "\n",
    "```bash\n",
    "papermill --log-output collect_build_times.ipynb collect_build_times.run1.ipynb\n",
    "\n",
    "# Dry run\n",
    "papermill --log-output collect_build_times.ipynb collect_build_times.run1.ipynb -p DRY_RUN 1\n",
    "\n",
    "# To override parameters with yaml:\n",
    "papermill --log-output collect_build_times.ipynb collect_build_times.run1.ipynb -y \"\n",
    "DATASETS:\n",
    "    - \"sift-128-euclidean\"\n",
    "\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import subprocess\n",
    "import time\n",
    "from typing import Callable\n",
    "\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Notebook parameters\n",
    "DRY_RUN = False\n",
    "RUN_E2E = False  # Run end-to-end benchmark\n",
    "\n",
    "DATASETS = [common.SIFT_128_EUCLIDEAN]\n",
    "ALGORITHMS = [\n",
    "    common.FAISS_CPU_FLAT,\n",
    "    common.FAISS_CPU_IVF_FLAT,\n",
    "    common.FAISS_CPU_IVF_PQ,\n",
    "    common.FAISS_GPU_FLAT,\n",
    "    common.FAISS_GPU_IVF_FLAT,\n",
    "    common.FAISS_GPU_IVF_PQ,\n",
    "    common.HNSWLIB,\n",
    "    common.RAFT_BRUTE_FORCE,\n",
    "    common.RAFT_CAGRA,\n",
    "    common.RAFT_IVF_FLAT,\n",
    "    common.RAFT_IVF_PQ,\n",
    "    common.RAFT_CAGRA_HNSWLIB,\n",
    "]\n",
    "DATASET_PATH = common.DATASET_PATH\n",
    "ALGO_CONFIG_DIR = common.ALGO_CONFIG_DIR\n",
    "GPU_DOCKER_CMD_TEMPLATE = common.GPU_DOCKER_CMD_TEMPLATE\n",
    "CPU_DOCKER_CMD_TEMPLATE = common.CPU_DOCKER_CMD_TEMPLATE\n",
    "DOWNLOAD_CMD_TEMPLATE = common.DOWNLOAD_CMD_TEMPLATE\n",
    "BUILD_CMD_TEMPLATE = common.BUILD_CMD_TEMPLATE\n",
    "SEARCH_CMD_TEMPLATE = common.SEARCH_CMD_TEMPLATE\n",
    "EXPORT_DATA_CMD_TEMPLATE = common.EXPORT_DATA_CMD_TEMPLATE\n",
    "PLOT_CMD_TEMPLATE = common.PLOT_CMD_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = pathlib.Path(DATASET_PATH).resolve()\n",
    "DATASET_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docker_image(algo: str = \"\"):\n",
    "    \"\"\"FAISS requires CUDA 11.\"\"\"\n",
    "    if algo.startswith(\"faiss\"):\n",
    "        return common.CUDA11_DOCKER_IMAGE\n",
    "    return common.CUDA12_DOCKER_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_collector(datasets: list[str], algorithms: list[str], default_fn: Callable):\n",
    "    coll = {}\n",
    "    for ds in datasets:\n",
    "        for algo in algorithms:\n",
    "            coll[(ds, algo)] = default_fn()\n",
    "    return coll\n",
    "\n",
    "\n",
    "build_time_collector = init_collector(\n",
    "    DATASETS, ALGORITHMS, lambda: dict(tick=None, tock=None)\n",
    ")\n",
    "search_time_collector = init_collector(\n",
    "    DATASETS, ALGORITHMS, lambda: dict(tick=None, tock=None)\n",
    ")\n",
    "error_collector = {ds: list() for ds in DATASETS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "\n",
    "* Use the CPU image for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_all():\n",
    "    for ds in DATASETS:\n",
    "        dataset_dir = DATASET_PATH / \"datasets\"\n",
    "        if DRY_RUN:\n",
    "            print(f\"Would download {ds} to {dataset_dir}\")\n",
    "            continue\n",
    "        try:\n",
    "            if ds == common.WIKI_ALL_1M:\n",
    "                common.download_wiki_1M(dataset_dir)\n",
    "                continue\n",
    "            if ds == common.WIKI_ALL_10M:\n",
    "                common.download_wiki_10M(dataset_dir)\n",
    "                continue\n",
    "            if ds == common.WIKI_ALL_88M:\n",
    "                common.download_wiki_88M(dataset_dir)\n",
    "                continue\n",
    "\n",
    "            need_normalize = ds in (common.GLOVE_100_INNER, common.DEEP_10M_INNER)\n",
    "            if need_normalize:\n",
    "                ds = ds.replace(\"inner\", \"angular\")\n",
    "\n",
    "            download_cmd = DOWNLOAD_CMD_TEMPLATE.safe_substitute(\n",
    "                DATASET=ds, NORMALIZE=\"--normalize\" if need_normalize else \"\"\n",
    "            )\n",
    "            cmd = CPU_DOCKER_CMD_TEMPLATE.safe_substitute(\n",
    "                DATASET_PATH=DATASET_PATH,\n",
    "                DOCKER_IMAGE=get_docker_image(),\n",
    "                CONTAINER_CMD=download_cmd,\n",
    "            )\n",
    "            print(cmd)\n",
    "            subprocess.run(\n",
    "                cmd, shell=True, executable=\"/bin/bash\", check=True, text=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {ds}: {e}\")\n",
    "            error_collector[ds].append(\"download\")\n",
    "            continue\n",
    "\n",
    "\n",
    "download_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all():\n",
    "    for ds in DATASETS:\n",
    "        for algo in ALGORITHMS:\n",
    "            extra_args = []\n",
    "            # FIXME: the docker image is either missing configurations, or the default doesn't work\n",
    "            need_algo_config = algo in (\n",
    "                common.FAISS_CPU_IVF_FLAT,\n",
    "                common.FAISS_CPU_IVF_PQ,\n",
    "            )\n",
    "            if need_algo_config:\n",
    "                extra_args.append(f\"--configuration {ALGO_CONFIG_DIR}\")\n",
    "\n",
    "            # FIXME: some algos need --force to write results properly\n",
    "            need_force_write = algo in (common.FAISS_GPU_FLAT, common.RAFT_BRUTE_FORCE)\n",
    "            if need_force_write:\n",
    "                extra_args.append(\"--force\")\n",
    "\n",
    "            build_cmd = BUILD_CMD_TEMPLATE.safe_substitute(\n",
    "                DATASET=ds, ALGORITHMS=algo, EXTRA_ARGS=\" \".join(extra_args)\n",
    "            )\n",
    "            cmd = GPU_DOCKER_CMD_TEMPLATE.safe_substitute(\n",
    "                DATASET_PATH=DATASET_PATH,\n",
    "                DOCKER_IMAGE=get_docker_image(algo),\n",
    "                CONTAINER_CMD=build_cmd,\n",
    "            )\n",
    "            print(cmd)\n",
    "            if DRY_RUN:\n",
    "                print(f\"Would build {ds} with {algo}\")\n",
    "                continue\n",
    "            try:\n",
    "                build_time_collector[(ds, algo)][\"tick\"] = time.time()\n",
    "                subprocess.run(\n",
    "                    cmd, shell=True, executable=\"/bin/bash\", check=True, text=True\n",
    "                )\n",
    "                build_time_collector[(ds, algo)][\"tock\"] = time.time()\n",
    "            except Exception as e:\n",
    "                print(f\"Error building {ds} with {algo}: {e}\")\n",
    "                error_collector[ds].append(f\"{algo} build\")\n",
    "                continue\n",
    "\n",
    "\n",
    "build_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "* Use the `latency` search mode, and set `--search-threads=1` as we only need this for correlating the build time statistics to recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_all():\n",
    "    for ds in DATASETS:\n",
    "        for algo in ALGORITHMS:\n",
    "            extra_args = []\n",
    "            # FIXME: the docker image is missing configurations for FAISS_CPU_IVF_FLAT and FAISS_CPU_IVF_PQ\n",
    "            need_algo_config = algo in (\n",
    "                common.FAISS_CPU_IVF_FLAT,\n",
    "                common.FAISS_CPU_IVF_PQ,\n",
    "            )\n",
    "            if need_algo_config:\n",
    "                extra_args.append(f\"--configuration {ALGO_CONFIG_DIR}\")\n",
    "            extra_args.append(\"--search-threads 1\")\n",
    "\n",
    "            search_cmd = SEARCH_CMD_TEMPLATE.safe_substitute(\n",
    "                SEARCH_MODE=\"latency\",\n",
    "                DATASET=ds,\n",
    "                ALGORITHMS=algo,\n",
    "                BATCH_SIZE=1,\n",
    "                COUNT=10,\n",
    "                EXTRA_ARGS=\" \".join(extra_args),\n",
    "            )\n",
    "            cmd = GPU_DOCKER_CMD_TEMPLATE.safe_substitute(\n",
    "                DATASET_PATH=DATASET_PATH,\n",
    "                DOCKER_IMAGE=get_docker_image(algo),\n",
    "                CONTAINER_CMD=search_cmd,\n",
    "            )\n",
    "            print(cmd)\n",
    "            if DRY_RUN:\n",
    "                print(f\"Would search {ds} with {algo}\")\n",
    "                continue\n",
    "            try:\n",
    "                search_time_collector[(ds, algo)][\"tick\"] = time.time()\n",
    "                subprocess.run(\n",
    "                    cmd, shell=True, executable=\"/bin/bash\", check=True, text=True\n",
    "                )\n",
    "\n",
    "                search_time_collector[(ds, algo)][\"tock\"] = time.time()\n",
    "            except Exception as e:\n",
    "                print(f\"Error searching {ds} with {algo}: {e}\")\n",
    "                error_collector[ds].append(f\"{algo} search\")\n",
    "                continue\n",
    "\n",
    "\n",
    "if RUN_E2E:\n",
    "    search_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_all():\n",
    "    for ds in DATASETS:\n",
    "        export_cmd = EXPORT_DATA_CMD_TEMPLATE.safe_substitute(DATASET=ds)\n",
    "        cmd = GPU_DOCKER_CMD_TEMPLATE.safe_substitute(\n",
    "            DATASET_PATH=DATASET_PATH,\n",
    "            DOCKER_IMAGE=get_docker_image(),\n",
    "            CONTAINER_CMD=export_cmd,\n",
    "        )\n",
    "        print(cmd)\n",
    "        if DRY_RUN:\n",
    "            print(f\"Would export {ds} from {DATASET_PATH}\")\n",
    "            continue\n",
    "        try:\n",
    "            subprocess.run(\n",
    "                cmd, shell=True, executable=\"/bin/bash\", check=True, text=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error exporting {ds}: {e}\")\n",
    "            error_collector[ds].append(\"export\")\n",
    "            continue\n",
    "\n",
    "\n",
    "if RUN_E2E:\n",
    "    export_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all():\n",
    "    for ds in DATASETS:\n",
    "        export_cmd = PLOT_CMD_TEMPLATE.safe_substitute(\n",
    "            SEARCH_MODE=\"latency\",\n",
    "            DATASET=ds,\n",
    "            BATCH_SIZE=1,  # This should be the same as in search_all\n",
    "            COUNT=10,  # This should be the same as in search_all\n",
    "            EXTRA_ARGS=\"\",\n",
    "        )\n",
    "        cmd = GPU_DOCKER_CMD_TEMPLATE.safe_substitute(\n",
    "            DATASET_PATH=DATASET_PATH,\n",
    "            DOCKER_IMAGE=get_docker_image(),\n",
    "            CONTAINER_CMD=export_cmd,\n",
    "        )\n",
    "        print(cmd)\n",
    "        if DRY_RUN:\n",
    "            print(f\"Would plot {ds} from {DATASET_PATH}\")\n",
    "            continue\n",
    "        try:\n",
    "            subprocess.run(\n",
    "                cmd, shell=True, executable=\"/bin/bash\", check=True, text=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting {ds}: {e}\")\n",
    "            error_collector[ds].append(\"plot\")\n",
    "            continue\n",
    "\n",
    "\n",
    "if RUN_E2E:\n",
    "    plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_collector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in build_time_collector.items():\n",
    "    build_time = (\n",
    "        round(v[\"tock\"] - v[\"tick\"], 2) if v[\"tick\"] and v[\"tock\"] else \"failed\"\n",
    "    )\n",
    "    print(f\"Build time for {k}: {build_time}\")\n",
    "\n",
    "\n",
    "if RUN_E2E:\n",
    "    for k, v in search_time_collector.items():\n",
    "        search_time = (\n",
    "            round(v[\"tock\"] - v[\"tick\"], 2) if v[\"tick\"] and v[\"tock\"] else \"failed\"\n",
    "        )\n",
    "        print(f\"Search time for {k}: {search_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
