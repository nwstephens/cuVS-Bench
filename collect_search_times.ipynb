{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Search Times\n",
    "\n",
    "Collect search times for datasets.\n",
    "\n",
    "How to run\n",
    "\n",
    "```bash\n",
    "papermill --log-output collect_search_times.ipynb collect_search_times.run1.ipynb\n",
    "\n",
    "# Dry run\n",
    "papermill --log-output collect_search_times.ipynb collect_search_times.run1.ipynb -p DRY_RUN 1\n",
    "\n",
    "# To override parameters with yaml:\n",
    "papermill --log-output collect_search_times.ipynb collect_search_times.run1.ipynb -y \"\n",
    "SEARCH_MODE: throughput\n",
    "DATASETS:\n",
    "    - \"sift-128-euclidean\"\n",
    "\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Notebook parameters\n",
    "DRY_RUN = False\n",
    "DATASETS = [common.SIFT_128_EUCLIDEAN]\n",
    "ALGORITHMS = [\n",
    "    common.FAISS_CPU_FLAT,\n",
    "    common.FAISS_CPU_IVF_FLAT,\n",
    "    common.FAISS_CPU_IVF_PQ,\n",
    "    common.FAISS_GPU_FLAT,\n",
    "    common.FAISS_GPU_IVF_FLAT,\n",
    "    common.FAISS_GPU_IVF_PQ,\n",
    "    common.HNSWLIB,\n",
    "    common.RAFT_BRUTE_FORCE,\n",
    "    common.RAFT_CAGRA,\n",
    "    common.RAFT_IVF_FLAT,\n",
    "    common.RAFT_IVF_PQ,\n",
    "    common.RAFT_CAGRA_HNSWLIB,\n",
    "]\n",
    "COUNTS = [10, 100]\n",
    "SEARCH_MODE = \"latency\"  # one of (\"latency\", \"throughput\").\n",
    "BATCH_SIZES = {\n",
    "    \"latency\": [1],\n",
    "    \"throughput\": [1, 10, 100, 10000],\n",
    "}\n",
    "SEARCH_THREADS = {\n",
    "    common.FAISS_GPU_FLAT: \"1\",\n",
    "    common.FAISS_GPU_IVF_FLAT: \"1:4\",\n",
    "    common.FAISS_GPU_IVF_PQ: \"1:4\",\n",
    "    common.FAISS_CPU_FLAT: \"1\",\n",
    "    common.FAISS_CPU_IVF_FLAT: \"1:4\",\n",
    "    common.FAISS_CPU_IVF_PQ: \"1:4\",\n",
    "    common.RAFT_BRUTE_FORCE: \"1\",\n",
    "    common.RAFT_CAGRA: \"1:16\",\n",
    "    common.RAFT_IVF_FLAT: \"1:4\",\n",
    "    common.RAFT_IVF_PQ: \"1:4\",\n",
    "}\n",
    "SEARCH_THREADS_BY_BATCH_SIZE = {\n",
    "    10000: \"1\",\n",
    "}\n",
    "DATASET_PATH = common.DATASET_PATH\n",
    "ALGO_CONFIG_DIR = common.ALGO_CONFIG_DIR\n",
    "GPU_DOCKER_CMD_TEMPLATE = common.GPU_DOCKER_CMD_TEMPLATE\n",
    "CPU_DOCKER_CMD_TEMPLATE = common.CPU_DOCKER_CMD_TEMPLATE\n",
    "DOWNLOAD_CMD_TEMPLATE = common.DOWNLOAD_CMD_TEMPLATE\n",
    "BUILD_CMD_TEMPLATE = common.BUILD_CMD_TEMPLATE\n",
    "SEARCH_CMD_TEMPLATE = common.SEARCH_CMD_TEMPLATE\n",
    "EXPORT_DATA_CMD_TEMPLATE = common.EXPORT_DATA_CMD_TEMPLATE\n",
    "PLOT_CMD_TEMPLATE = common.PLOT_CMD_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = pathlib.Path(DATASET_PATH).resolve()\n",
    "DATASET_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docker_image(algo: str = \"\"):\n",
    "    \"\"\"FAISS requires CUDA 11.\"\"\"\n",
    "    if algo.startswith(\"faiss\"):\n",
    "        return common.CUDA11_DOCKER_IMAGE\n",
    "    return common.CUDA12_DOCKER_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_params():\n",
    "    \"\"\"An iterator over all parameter combinations.\"\"\"\n",
    "    for ds in DATASETS:\n",
    "        for algo in ALGORITHMS:\n",
    "            for bs in BATCH_SIZES[SEARCH_MODE]:\n",
    "                for cnt in COUNTS:\n",
    "                    yield ds, algo, bs, cnt\n",
    "\n",
    "\n",
    "search_time_collector = {\n",
    "    (ds, algo, bs, cnt): dict(tick=None, tock=None)\n",
    "    for ds, algo, bs, cnt in iter_params()\n",
    "}\n",
    "error_collector = {ds: list() for ds in DATASETS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "* Searching takes a while, so make sure that we don't do the same search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_result_path(ds: str, algo: str, bs: int, cnt: str):\n",
    "    return (\n",
    "        DATASET_PATH\n",
    "        / \"datasets\"\n",
    "        / ds\n",
    "        / \"result\"\n",
    "        / \"search\"\n",
    "        / f\"{algo},base,k{cnt},bs{bs}.json\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_threads(algo: str | None = None, bs: int | None = None) -> str | None:\n",
    "    for candidate in [SEARCH_THREADS_BY_BATCH_SIZE.get(bs), SEARCH_THREADS.get(algo)]:\n",
    "        if candidate:\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "\n",
    "def search_all():\n",
    "    for ds, algo, bs, cnt in iter_params():\n",
    "        res_file_path = get_search_result_path(ds, algo, bs, cnt)\n",
    "        if res_file_path.exists():\n",
    "            print(f\"Skipping {res_file_path} as it already exists.\")\n",
    "            continue\n",
    "\n",
    "        extra_args = []\n",
    "        # FIXME: the docker image is missing configurations for FAISS_CPU_IVF_FLAT and FAISS_CPU_IVF_PQ\n",
    "        need_algo_config = algo in (common.FAISS_CPU_IVF_FLAT, common.FAISS_CPU_IVF_PQ)\n",
    "        if need_algo_config:\n",
    "            extra_args.append(f\"--configuration {ALGO_CONFIG_DIR}\")\n",
    "\n",
    "        # Find the first suitable the number of threads to use\n",
    "        threads = get_threads(algo=algo, bs=bs)\n",
    "        if threads:\n",
    "            extra_args.append(f\"--search-threads {threads}\")\n",
    "\n",
    "        search_cmd = SEARCH_CMD_TEMPLATE.safe_substitute(\n",
    "            SEARCH_MODE=SEARCH_MODE,\n",
    "            DATASET=ds,\n",
    "            ALGORITHMS=algo,\n",
    "            BATCH_SIZE=bs,\n",
    "            COUNT=cnt,\n",
    "            EXTRA_ARGS=\" \".join(extra_args),\n",
    "        )\n",
    "        cmd = GPU_DOCKER_CMD_TEMPLATE.safe_substitute(\n",
    "            DATASET_PATH=DATASET_PATH,\n",
    "            DOCKER_IMAGE=get_docker_image(algo),\n",
    "            CONTAINER_CMD=search_cmd,\n",
    "        )\n",
    "        print(cmd)\n",
    "        if DRY_RUN:\n",
    "            print(f\"Would search {ds} with {algo}\")\n",
    "            continue\n",
    "        try:\n",
    "            search_time_collector[(ds, algo, bs, cnt)][\"tick\"] = time.time()\n",
    "            subprocess.run(\n",
    "                cmd, shell=True, executable=\"/bin/bash\", check=True, text=True\n",
    "            )\n",
    "            search_time_collector[(ds, algo, bs, cnt)][\"tock\"] = time.time()\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching {ds} with {algo}: {e}\")\n",
    "            error_collector[ds].append(f\"search {algo} {SEARCH_MODE} bs{bs} k{cnt}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "search_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_all():\n",
    "    for ds in DATASETS:\n",
    "        export_cmd = EXPORT_DATA_CMD_TEMPLATE.safe_substitute(DATASET=ds)\n",
    "        cmd = GPU_DOCKER_CMD_TEMPLATE.safe_substitute(\n",
    "            DATASET_PATH=DATASET_PATH,\n",
    "            DOCKER_IMAGE=get_docker_image(),\n",
    "            CONTAINER_CMD=export_cmd,\n",
    "        )\n",
    "        print(cmd)\n",
    "        if DRY_RUN:\n",
    "            print(f\"Would export {ds} from {DATASET_PATH}\")\n",
    "            continue\n",
    "        try:\n",
    "            subprocess.run(\n",
    "                cmd, shell=True, executable=\"/bin/bash\", check=True, text=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error exporting {ds}: {e}\")\n",
    "            error_collector[ds].append(\"export\")\n",
    "            continue\n",
    "\n",
    "\n",
    "export_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive results\n",
    "\n",
    "We need to archive the search results as they can be overwritten when we switch search mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_search_results():\n",
    "    for ds, _, _, _ in iter_params():\n",
    "        src_dir = DATASET_PATH / \"datasets\" / ds / \"result\" / \"search\"\n",
    "        dest_dir = DATASET_PATH / \"datasets\" / ds / \"result\" / f\"search_{SEARCH_MODE}\"\n",
    "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "        for src in src_dir.glob(\"*.json\"):\n",
    "            dest = dest_dir / src.name\n",
    "            if DRY_RUN:\n",
    "                print(f\"Would copy {src} to {dest}\")\n",
    "                continue\n",
    "            shutil.copy(src, dest)\n",
    "        for src in src_dir.glob(\"*.csv\"):\n",
    "            dest = dest_dir / src.name\n",
    "            if DRY_RUN:\n",
    "                print(f\"Would copy {src} to {dest}\")\n",
    "                continue\n",
    "            shutil.copy(src, dest)\n",
    "\n",
    "\n",
    "copy_search_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all():\n",
    "    for ds in DATASETS:\n",
    "        export_cmd = PLOT_CMD_TEMPLATE.safe_substitute(\n",
    "            SEARCH_MODE=\"latency\",\n",
    "            DATASET=ds,\n",
    "            BATCH_SIZE=BATCH_SIZES[SEARCH_MODE][0],\n",
    "            COUNT=COUNTS[0],\n",
    "            EXTRA_ARGS=\"\",\n",
    "        )\n",
    "        cmd = GPU_DOCKER_CMD_TEMPLATE.safe_substitute(\n",
    "            DATASET_PATH=DATASET_PATH,\n",
    "            DOCKER_IMAGE=get_docker_image(),\n",
    "            CONTAINER_CMD=export_cmd,\n",
    "        )\n",
    "        print(cmd)\n",
    "        if DRY_RUN:\n",
    "            print(f\"Would plot {ds} from {DATASET_PATH}\")\n",
    "            continue\n",
    "        try:\n",
    "            subprocess.run(\n",
    "                cmd, shell=True, executable=\"/bin/bash\", check=True, text=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting {ds}: {e}\")\n",
    "            error_collector[ds].append(\"plot\")\n",
    "            continue\n",
    "\n",
    "\n",
    "# FIXME: plot does not always work.\n",
    "# plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_collector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in search_time_collector.items():\n",
    "    search_time = (\n",
    "        round(v[\"tock\"] - v[\"tick\"], 2) if v[\"tick\"] and v[\"tock\"] else \"failed\"\n",
    "    )\n",
    "    print(f\"Search time for {k}: {search_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
